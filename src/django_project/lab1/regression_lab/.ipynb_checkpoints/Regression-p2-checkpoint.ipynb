{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nicholas/Documents/Machine Learning\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression # model for logistic regression\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "#path = \"D:\\home\\nicholas\\Documents\\Machine Learning\"\n",
    "#os.chdir(path)\n",
    "print(os.getcwd())\n",
    "\n",
    "'''\n",
    "MYCT: Machine Cycle Time\n",
    "MMIN: Memory Min\n",
    "MMAX: Memory Max\n",
    "CACH: Cache Size\n",
    "CHMIN: Channel Min\n",
    "CHMAX: Channel Max\n",
    "'''\n",
    "#print(X_test)\n",
    "# Fit model and predict test values\n",
    "# y_pred = np.random.randint(y_train.min(), y_train.max(), X_test.shape[0])\n",
    "\n",
    "\n",
    "def LinRegTrans(X_train, y_train, X_test):\n",
    "    regressor = LinearRegression()\n",
    "    sc = StandardScaler()\n",
    "    X_train_std = sc.fit_transform(X_train)\n",
    "    X_test_std = sc.transform(X_test)\n",
    "    #transformer = QuantileTransformer(output_distribution='normal')\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(X_train_std, y_train)\n",
    "    y_pred = reg.predict(X_test_std)\n",
    "    print(\"Reg Score:\",reg.score(X_train_std, y_train))\n",
    "    #print(\"Coeff:\",reg.coef_)\n",
    "    #print(\"Intercept:\",reg.intercept_)\n",
    "    '''\n",
    "    for i in range(len(y_pred)):\n",
    "        print(\"Index:\",i,X_test[i],y_pred[i])\n",
    "    '''\n",
    "    return(y_pred)\n",
    "\n",
    "def LinReg(X_train, y_train, X_test):\n",
    "    reg = LinearRegression().fit(X_train, y_train)\n",
    "    y_pred = reg.predict(X_test)\n",
    "    print(\"Reg Score:\",reg.score(X_train, y_train))\n",
    "    print(\"Coeff:\",reg.coef_)\n",
    "    print(\"Intercept:\",reg.intercept_)\n",
    "    return(y_pred)\n",
    "\n",
    "def RandomForestReg(X_train, y_train, X_test):\n",
    "    reg = RandomForestRegressor(n_estimators=300000)\n",
    "    reg.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = reg.predict(X_test)\n",
    "    print(\"Reg Score:\",reg.score(X_train, y_train))\n",
    "    #print(\"Coeff:\",reg.coef_)\n",
    "    #print(\"Intercept:\",reg.intercept_)\n",
    "    return(y_pred)\n",
    "\n",
    "\n",
    "def LinRegTransAlt(X_train, y_train, X_test):\n",
    "    std = StandardScaler()\n",
    "    reg = LinearRegression().fit(X_train, y_train)\n",
    "    y_pred = reg.predict(X_test)\n",
    "    print(\"Reg Score:\",reg.score(X_train, y_train))\n",
    "    print(\"Coeff:\",reg.coef_)\n",
    "    print(\"Intercept:\",reg.intercept_)\n",
    "    return(y_pred)\n",
    "\n",
    "def LogRegression(X_train, y_train, X_test):\n",
    "    logisticRegr = LogisticRegression(solver='saga', multi_class='auto')\n",
    "    logisticRegr.fit(X_train, y_train)\n",
    "    y_pred = logisticRegr.predict(X_test)\n",
    "    print(\"Reg Score:\",logisticRegr.score(X_train, y_train))\n",
    "    #print(\"Coeff:\",logisticRegr.coef_)\n",
    "    #print(\"Intercept:\",logisticRegr.intercept_)\n",
    "    #return(y_pred)\n",
    "    \n",
    "# Arrange answer in two columns. First column (with header \"Id\") is an\n",
    "# enumeration from 0 to n-1, where n is the number of test points. Second\n",
    "# column (with header \"EpiOrStroma\" is the predictions.\n",
    "def saveFile(y_pred,name):\n",
    "    test_header = \"Id,PRP\"\n",
    "    n_points = X_test.shape[0]\n",
    "    y_pred_pp = np.ones((n_points, 2))\n",
    "    y_pred_pp[:, 0] = range(n_points)\n",
    "    y_pred_pp[:, 1] = y_pred\n",
    "    np.savetxt(name, y_pred_pp, fmt='%d,%f', delimiter=\",\",\n",
    "               header=test_header, comments=\"\")\n",
    "\n",
    "# Note: fmt='%d' denotes that all values should be formatted as integers which\n",
    "# is appropriate for classification. For regression, where the second column\n",
    "# should be floating point, use fmt='%d,%f'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MYCT: Machine Cycle Time\n",
    "MMIN: Memory Min\n",
    "MMAX: Memory Max\n",
    "CACH: Cache Size\n",
    "CHMIN: Channel Min\n",
    "CHMAX: Channel Max\n",
    "'''\n",
    "\n",
    "# Load training and testing data\n",
    "X_train = np.loadtxt('/home/nicholas/Documents/Machine Learning/X_train.csv', delimiter=',', skiprows=1)\n",
    "X_test = np.loadtxt('/home/nicholas/Documents/Machine Learning/X_test.csv', delimiter=',', skiprows=1)\n",
    "y_train = np.loadtxt('/home/nicholas/Documents/Machine Learning/y_train.csv', delimiter=',', skiprows=1)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[ 269.  220.  172.  132.  318.  636. 1144.   40.   92.  138.   10.   35.\n   19.   28.  120.   33.   61.   76.   23.   69.   33.   27.   77.   27.\n  274.  368.   32.   63.   29.   71.   26.   40.   60.   72.   72.   20.\n   40.   62.   24.  138.   26.   60.   12.   14.   20.   16.   36.  144.\n  144.  259.   17.   26.   32.   32.   62.   64.   22.   36.   44.   50.\n   45.   36.   84.   16.   38.   16.   22.   29.   40.   35.   66.  141.\n  189.   22.  132.  465.  465.  277.  185.    6.   24.   45.    7.   13.\n   16.   32.   11.   11.   18.   22.   37.   40.   34.   50.   76.   66.\n   24.   49.   66.  100.   12.   18.   27.   56.   70.   80.  136.   16.\n   26.   32.   54.   65.   50.   40.   62.   60.   50.   66.   86.   74.\n   93.  111.  143.  214.  277.  370.  510.  326.  510.   12.   17.   24.\n   34.   42.   46.  100.  140.  212.   25.   30.   50.   50.   30.   32.\n   38.   60.   11.   22.   33.   58.  130.   75.  113.  188.  173.  405.\n   70.  114.  208.  397.  915.   12.   14.   18.   21.   46.   52.   67.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-a0c08c3da800>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_train_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_test_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0my_train_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#####################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    756\u001b[0m         X = check_array(X, accept_sparse='csr', copy=copy,\n\u001b[1;32m    757\u001b[0m                         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m                         force_all_finite='allow-nan')\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    519\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[ 269.  220.  172.  132.  318.  636. 1144.   40.   92.  138.   10.   35.\n   19.   28.  120.   33.   61.   76.   23.   69.   33.   27.   77.   27.\n  274.  368.   32.   63.   29.   71.   26.   40.   60.   72.   72.   20.\n   40.   62.   24.  138.   26.   60.   12.   14.   20.   16.   36.  144.\n  144.  259.   17.   26.   32.   32.   62.   64.   22.   36.   44.   50.\n   45.   36.   84.   16.   38.   16.   22.   29.   40.   35.   66.  141.\n  189.   22.  132.  465.  465.  277.  185.    6.   24.   45.    7.   13.\n   16.   32.   11.   11.   18.   22.   37.   40.   34.   50.   76.   66.\n   24.   49.   66.  100.   12.   18.   27.   56.   70.   80.  136.   16.\n   26.   32.   54.   65.   50.   40.   62.   60.   50.   66.   86.   74.\n   93.  111.  143.  214.  277.  370.  510.  326.  510.   12.   17.   24.\n   34.   42.   46.  100.  140.  212.   25.   30.   50.   50.   30.   32.\n   38.   60.   11.   22.   33.   58.  130.   75.  113.  188.  173.  405.\n   70.  114.  208.  397.  915.   12.   14.   18.   21.   46.   52.   67.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# SCALING\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "y_train_std = sc.transform(y_train)\n",
    "\n",
    "#####################################################################\n",
    "## COMBINING FOR FUTURE PLOTTING\n",
    "\n",
    "#X_combined_sepal_standard = np.vstack((X_train_sepal_std,X_test_sepal_std))\n",
    "#Y_combined_sepal = np.hstack((y_train_sepal, y_test_sepal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substracting Max & Min\n",
    "X_train2 = np.zeros((len(X_train),4))\n",
    "X_test2 = np.zeros((len(X_test),4))\n",
    "\n",
    "X_train2[:,0] = X_train[:,0]\n",
    "X_train2[:,1] = X_train[:,2]-X_train[:,1]\n",
    "X_train2[:,2] = X_train[:,3]\n",
    "X_train2[:,3] = X_train[:,5]-X_train[:,4]\n",
    "\n",
    "X_test2[:,0] = X_test[:,0]\n",
    "X_test2[:,1] = X_test[:,2]-X_test[:,1]\n",
    "X_test2[:,2] = X_test[:,3]\n",
    "X_test2[:,3] = X_test[:,5]-X_test[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging Max & Min\n",
    "X_train3 = np.zeros((len(X_train),4))\n",
    "X_test3 = np.zeros((len(X_test),4))\n",
    "\n",
    "X_train3[:,0] = X_train[:,0]\n",
    "X_train3[:,1] = (X_train[:,2]+X_train[:,1])/2\n",
    "X_train3[:,2] = X_train[:,3]\n",
    "X_train3[:,3] = (X_train[:,5]+X_train[:,4])/2\n",
    "\n",
    "X_test3[:,0] = X_test[:,0]\n",
    "X_test3[:,1] = (X_test[:,2]+X_test[:,1])/2\n",
    "X_test3[:,2] = X_test[:,3]\n",
    "X_test3[:,3] = (X_test[:,5]+X_test[:,4])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "meansMemory = []\n",
    "meansChannel = []\n",
    "for index in range (len(X_train)):\n",
    "    meansMemory += [(X_train[index,2]+X_train[index,1])/2]\n",
    "    meansChannel += [(X_train[index,5]+X_train[index,4])/2]\n",
    "    \n",
    "totalMeanMem = np.sum(meansMemory)/len(X_train)\n",
    "totalMeanChan = np.sum(meansChannel)/len(X_train)\n",
    "\n",
    "meansMemoryTest = []\n",
    "meansChannelTest = []\n",
    "for index in range (len(X_test)):\n",
    "    meansMemoryTest += [(X_test[index,2]+X_test[index,1])/2]\n",
    "    meansChannelTest += [(X_test[index,5]+X_test[index,4])/2]\n",
    "    \n",
    "totalMeanMemTest = np.sum(meansMemoryTest)/len(X_test)\n",
    "totalMeanChanTest = np.sum(meansChannelTest)/len(X_test)\n",
    "\n",
    "# Difference to overall mean\n",
    "X_train4 = np.zeros((len(X_train),4))\n",
    "X_test4 = np.zeros((len(X_test),4))\n",
    "\n",
    "X_train4[:,0] = X_train[:,0]\n",
    "X_train4[:,1] = ((X_train[:,2]+X_train[:,1])/2)-totalMeanMem\n",
    "X_train4[:,2] = X_train[:,3]\n",
    "X_train4[:,3] = ((X_train[:,5]+X_train[:,4])/2)-totalMeanChan\n",
    "\n",
    "X_test4[:,0] = X_test[:,0]\n",
    "X_test4[:,1] = ((X_test[:,2]+X_test[:,1])/2)-totalMeanMemTest\n",
    "X_test4[:,2] = X_test[:,4]\n",
    "X_test4[:,3] = ((X_test[:,5]+X_test[:,4])/2)-totalMeanChanTest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Model\n",
      "Reg Score: 0.8843164482932035\n",
      "Coeff: [0.04015267 0.01651999 0.00395823 0.66379148 1.94710236 1.31203166]\n",
      "Intercept: -46.44254789859781\n",
      "\n",
      "Substracting Min Max\n",
      "Reg Score: 0.7114808912829821\n",
      "Coeff: [0.01669637 0.00871668 1.76242411 0.62207142]\n",
      "Intercept: -28.107018813244082\n",
      "\n",
      "Averaging Min Max\n",
      "Reg Score: 0.8611515357038555\n",
      "Coeff: [0.04237978 0.01400863 0.90277442 2.02963181]\n",
      "Intercept: -51.57718553053502\n",
      "\n",
      "Average difference to overall mean\n",
      "Reg Score: 0.8611515357038555\n",
      "Coeff: [0.04237978 0.01400863 0.90277442 2.02963181]\n",
      "Intercept: 70.91458778404547\n",
      "\n",
      "=============================================================\n",
      "\n",
      "Transformed Data - Original\n",
      "Reg Score: 0.8843164482932037\n",
      "\n",
      "Transformed Data - Average difference to overall mean\n",
      "Reg Score: 0.8611515357038555\n",
      "\n",
      "Transformed Data - Average of Min Max\n",
      "Reg Score: 0.8611515357038554\n",
      "\n",
      "=============================================================\n",
      "\n",
      "Random Forest - Original\n",
      "Reg Score: 0.9737101603820764\n",
      "\n",
      "=============================================================\n",
      "\n",
      "Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "# Kaggle 59.77012\n",
    "print(\"\\nOriginal Model\")\n",
    "y_pred = LinReg(X_train, y_train, X_test)\n",
    "saveFile(y_pred,'my_submission.csv')\n",
    "\n",
    "print(\"\\nSubstracting Min Max\")\n",
    "y_pred = LinReg(X_train2, y_train, X_test2)\n",
    "saveFile(y_pred,'my_submission2.csv')\n",
    "\n",
    "# Kaggle 139.60341\n",
    "print(\"\\nAveraging Min Max\")\n",
    "y_pred = LinReg(X_train3, y_train, X_test3)\n",
    "saveFile(y_pred,'my_submission3.csv')\n",
    "\n",
    "# Kaggle 66.78272\n",
    "print(\"\\nAverage difference to overall mean\")\n",
    "y_pred = LinReg(X_train4, y_train, X_test4)\n",
    "saveFile(y_pred,'my_submission4.csv')\n",
    "\n",
    "\n",
    "print(\"\\n=============================================================\")\n",
    "# Kaggle 63.79880\n",
    "print(\"\\nTransformed Data - Original\")\n",
    "y_pred = LinRegTrans(X_train, y_train, X_test)\n",
    "saveFile(y_pred,'my_submission5.csv')\n",
    "\n",
    "\n",
    "print(\"\\nTransformed Data - Average difference to overall mean\")\n",
    "y_pred = LinRegTrans(X_train4, y_train, X_test4)\n",
    "saveFile(y_pred,'my_submission5.csv')\n",
    "\n",
    "print(\"\\nTransformed Data - Average of Min Max\")\n",
    "y_pred = LinRegTrans(X_train3, y_train, X_test3)\n",
    "saveFile(y_pred,'my_submission4.csv')\n",
    "\n",
    "print(\"\\n=============================================================\")\n",
    "print(\"\\nRandom Forest - Original\")\n",
    "# 10 Trees:      0.967439199962173\n",
    "# 100 Trees:     0.9736151477267015\n",
    "# 1000 Trees:    0.9743113004361875\n",
    "# 100000 Trees : 0.973428421182665\n",
    "y_pred = RandomForestReg(X_train_std, y_train, X_test_std)\n",
    "saveFile(y_pred,'my_submission6.csv')\n",
    "\n",
    "print(\"\\n=============================================================\")\n",
    "print(\"\\nLogistic Regression\")\n",
    "#y_pred = LogRegression(X_train_std, y_train, X_test_std)\n",
    "#saveFile(y_pred,'my_submission_logisticRegression.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
