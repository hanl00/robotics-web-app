{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168, 6)\n",
      "(41, 6)\n",
      "(168,)\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import linear_model, metrics\n",
    "from sklearn.metrics import (mean_squared_error, r2_score, mean_absolute_error, \n",
    "mean_squared_log_error, explained_variance_score, max_error)\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score, cross_val_predict\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Load training and testing data\n",
    "X_train = np.loadtxt(\"X_train.csv\", delimiter=',', skiprows=1)\n",
    "X_test = np.loadtxt(\"X_test.csv\", delimiter=',', skiprows=1)\n",
    "y_train = np.loadtxt(\"y_train.csv\", delimiter=',', skiprows=1)[:,1]\n",
    "\n",
    "#converting csv into pandas dataframe\n",
    "#X_train = pd.read_csv('X_train.csv') \n",
    "#X_test = pd.read_csv('X_test.csv')\n",
    "#y_train = pd.read_csv('y_train.csv')\n",
    "\n",
    "#drop rows are either CHMIN\n",
    "#row_0_values = (X_train != 0).all(1)\n",
    "#print (row_0_values)\n",
    "#X_train = X_train[(X_train != 0).all(1)]\n",
    "#y_train = y_train[row_0_values]\n",
    "#print (X_train.size)\n",
    "\n",
    "\n",
    "#scaling \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange answer in two columns. First column (with header \"Id\") is an\n",
    "# enumeration from 0 to n-1, where n is the number of test points. Second\n",
    "# column (with header \"EpiOrStroma\" is the predictions.\n",
    "def saveFile(y_pred,name):\n",
    "    test_header = \"Id,PRP\"\n",
    "    n_points = X_test.shape[0]\n",
    "    y_pred_pp = np.ones((n_points, 2))\n",
    "    y_pred_pp[:, 0] = range(n_points)\n",
    "    y_pred_pp[:, 1] = y_pred\n",
    "    np.savetxt(name, y_pred_pp, fmt='%d,%f', delimiter=\",\",\n",
    "               header=test_header, comments=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117, 6)\n",
      "(51, 6)\n",
      "(117,)\n",
      "(51,)\n",
      "(117, 6)\n",
      "(51, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#scaling \n",
    "X_train1_scaled = scaler.fit_transform(X_train1)\n",
    "X_test1_scaled = scaler.fit_transform(X_test1)\n",
    "\n",
    "print(X_train1.shape)\n",
    "print(X_test1.shape)\n",
    "print(y_train1.shape)\n",
    "print(y_test1.shape)\n",
    "\n",
    "print(X_train1_scaled.shape)\n",
    "print(X_test1_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining basic untuned regression models\n",
    "\n",
    "def basicLinearSVR(X_train1, X_test1, y_train1, y_test1):\n",
    "    svrReg = LinearSVR(max_iter = 9999)\n",
    "    svrReg.fit(X_train1, y_train1)\n",
    "    y_pred = svrReg.predict(X_test1)\n",
    "    print(\"R2 score: \" + str(svrReg.score(X_train1, y_train1)))\n",
    "    print(\"Explained variance: \" + str(explained_variance_score(y_test1, y_pred)))\n",
    "    print(\"Max error: \" + str(max_error(y_test1, y_pred)))\n",
    "    print(\"Mean absolute error: \" + str(mean_absolute_error(y_test1, y_pred)))\n",
    "    print(\"Mean squared error: \" + str(mean_squared_error(y_test1, y_pred)))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.4987626610928281\n",
      "Explained variance: 0.7011141050038857\n",
      "Max error: 238.94983682344952\n",
      "Mean absolute error: 32.45855616495046\n",
      "Mean squared error: 4071.193609463763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([177.57257888,  26.06481603,  97.7275538 ,  36.51143953,\n",
       "        14.58071053,  11.67462405,  12.85074128,  14.81806027,\n",
       "        67.45317143,  22.94882907,  50.24123997,  67.52160701,\n",
       "        55.32473952,  20.86191261,  31.00980144,  39.23762475,\n",
       "       111.73405068,  44.9826457 , 250.22523064,  41.96076227,\n",
       "         8.03876791,  66.24397322,  66.12704935,  19.69583589,\n",
       "        24.20308772,  95.53338736,   0.34458445,  57.95478516,\n",
       "       226.05016318,  63.50455474,  20.86191261,  19.8126588 ,\n",
       "        53.14689029,  86.36813971,  30.0054794 , 188.54504115,\n",
       "       177.46590645,  36.27324471,  67.110661  ,  85.92076595,\n",
       "        23.60950049,  47.17609156,  96.81708047,  22.92248754,\n",
       "       123.03719524,  48.03579627,   8.15906401,  26.47876035,\n",
       "       281.16989147,  17.63931925,  -2.28411582])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basicLinearSVR(X_train1_scaled, X_test1_scaled, y_train1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'C': 1.0, 'dual': True, 'epsilon': 0.0, 'fit_intercept': True, 'intercept_scaling': 1.0, 'loss': 'epsilon_insensitive', 'max_iter': 1000, 'random_state': None, 'tol': 0.0001, 'verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "# Linear Support Vector Regression - Further Tuning with Grid Search\n",
    "lsvr = LinearSVR()\n",
    "# Look at parameters used by our regression\n",
    "print('Parameters currently in use:\\n')\n",
    "print(lsvr.get_params())\n",
    "\n",
    "#Creating the Random Grid\n",
    "#pipe = Pipeline([('regression model' , LinearSVR())])\n",
    "\n",
    "param_grid = [\n",
    "    {#'regression' : [LinearSVR()],\n",
    "    'epsilon' : [0,75, 0.5, 0.25, 0],\n",
    "    'tol' : [1e-4, 2e-4],\n",
    "     'C' : np.linspace(0.5,100,200),\n",
    "    'fit_intercept' : [True, False],\n",
    "    #'intercept_scaling' : [0, 0.5, 1, 1.5, 2],\n",
    "    #'dual' : [True, False],\n",
    "    'verbose' : [0, 1],\n",
    "    'max_iter' : [500, 1000, 1500, 2000, 2500, 3000]}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.5   1.    1.5   2.    2.5   3.    3.5   4.    4.5   5.    5.5   6.\n",
      "   6.5   7.    7.5   8.    8.5   9.    9.5  10.   10.5  11.   11.5  12.\n",
      "  12.5  13.   13.5  14.   14.5  15.   15.5  16.   16.5  17.   17.5  18.\n",
      "  18.5  19.   19.5  20.   20.5  21.   21.5  22.   22.5  23.   23.5  24.\n",
      "  24.5  25.   25.5  26.   26.5  27.   27.5  28.   28.5  29.   29.5  30.\n",
      "  30.5  31.   31.5  32.   32.5  33.   33.5  34.   34.5  35.   35.5  36.\n",
      "  36.5  37.   37.5  38.   38.5  39.   39.5  40.   40.5  41.   41.5  42.\n",
      "  42.5  43.   43.5  44.   44.5  45.   45.5  46.   46.5  47.   47.5  48.\n",
      "  48.5  49.   49.5  50.   50.5  51.   51.5  52.   52.5  53.   53.5  54.\n",
      "  54.5  55.   55.5  56.   56.5  57.   57.5  58.   58.5  59.   59.5  60.\n",
      "  60.5  61.   61.5  62.   62.5  63.   63.5  64.   64.5  65.   65.5  66.\n",
      "  66.5  67.   67.5  68.   68.5  69.   69.5  70.   70.5  71.   71.5  72.\n",
      "  72.5  73.   73.5  74.   74.5  75.   75.5  76.   76.5  77.   77.5  78.\n",
      "  78.5  79.   79.5  80.   80.5  81.   81.5  82.   82.5  83.   83.5  84.\n",
      "  84.5  85.   85.5  86.   86.5  87.   87.5  88.   88.5  89.   89.5  90.\n",
      "  90.5  91.   91.5  92.   92.5  93.   93.5  94.   94.5  95.   95.5  96.\n",
      "  96.5  97.   97.5  98.   98.5  99.   99.5 100. ]\n"
     ]
    }
   ],
   "source": [
    "print(np.linspace(0.5,100,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 48000 candidates, totalling 480000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 12436 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done 35936 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-1)]: Done 68836 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done 111136 tasks      | elapsed:   38.8s\n",
      "[Parallel(n_jobs=-1)]: Done 162836 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 223936 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 294436 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 374336 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 463636 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 480000 out of 480000 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 6.5,\n",
       " 'epsilon': 0.5,\n",
       " 'fit_intercept': True,\n",
       " 'max_iter': 1500,\n",
       " 'tol': 0.0002,\n",
       " 'verbose': 1}"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model with the parameter\n",
    "reg = GridSearchCV(LinearSVR(), param_grid = param_grid, cv = 10, verbose=True, n_jobs=-1)\n",
    "# Fit the random search model\n",
    "best_reg= reg.fit(X_train1_scaled, y_train1)\n",
    "\n",
    "\n",
    "best_reg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned regression models\n",
    "\n",
    "def tunedLinearSVR(X_train1, X_test1, y_train1, y_test1):\n",
    "    svrReg = LinearSVR(C= 6.5, epsilon= 0.5, fit_intercept = True, max_iter = 1500, tol = 0.0002, verbose = 1 )\n",
    "    svrReg.fit(X_train1, y_train1)\n",
    "    y_pred = svrReg.predict(X_test1)\n",
    "    print(\"R2 score: \" + str(svrReg.score(X_test1, y_test1)))\n",
    "    print(\"Explained variance: \" + str(explained_variance_score(y_test1, y_pred)))\n",
    "    print(\"Max error: \" + str(max_error(y_test1, y_pred)))\n",
    "    print(\"Mean absolute error: \" + str(mean_absolute_error(y_test1, y_pred)))\n",
    "    print(\"Mean squared error: \" + str(mean_squared_error(y_test1, y_pred)))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]R2 score: 0.8020073197075291\n",
      "Explained variance: 0.8100024529525531\n",
      "Max error: 181.11212399785097\n",
      "Mean absolute error: 34.448176151688926\n",
      "Mean squared error: 2459.8676064244405\n"
     ]
    }
   ],
   "source": [
    "y_pred = tunedLinearSVR(X_train1_scaled, X_test1_scaled, y_train1, y_test1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8474789235030437"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svrReg = LinearSVR(C= 100, epsilon= 0.5, fit_intercept = True, max_iter = 3000, tol = 0.0002, verbose = 1 )\n",
    "svrReg.fit(X_train_scaled, y_train)\n",
    "y_pred = svrReg.predict(X_test_scaled)\n",
    "saveFile(y_pred,'svr_tuned_scaled.csv')\n",
    "svrReg.score(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic model for ridge regression\n",
    "rigReg = Ridge()\n",
    "rigReg.fit(X_train1, y_train1)\n",
    "\n",
    "def basicRidgeReg(X_train1, X_test1, y_train1, y_test1):\n",
    "    rigReg = Ridge()\n",
    "    rigReg.fit(X_train1, y_train1)\n",
    "    y_pred = rigReg.predict(X_test1)\n",
    "    print(\"R2 score: \" + str(rigReg.score(X_test1, y_test1)))\n",
    "    print(\"Explained variance: \" + str(explained_variance_score(y_test1, y_pred)))\n",
    "    print(\"Max error: \" + str(max_error(y_test1, y_pred)))\n",
    "    print(\"Mean absolute error: \" + str(mean_absolute_error(y_test1, y_pred)))\n",
    "    print(\"Mean squared error: \" + str(mean_squared_error(y_test1, y_pred)))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.7825060598137341\n",
      "Explained variance: 0.7897274648240552\n",
      "Max error: 186.00460404912445\n",
      "Mean absolute error: 35.13322383229487\n",
      "Mean squared error: 2702.151904138626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([244.67413306,   3.60428569, 120.32052783,  35.87954244,\n",
       "        -6.84974768, -14.31132657,  -9.97508057, -14.93014543,\n",
       "        83.55025995,  28.89142762,  69.74994622,  90.59002238,\n",
       "        86.38847148,  31.57949605,  24.27283306,  75.07755755,\n",
       "       125.92698484,  36.02575197, 465.84092056,  34.7669558 ,\n",
       "        -9.67020154, 113.63825911,  73.2928649 ,   9.80244081,\n",
       "         5.86985328, 150.25376163,  17.00606619,  60.49107861,\n",
       "       455.77055124, 112.06271361,  31.57949605,   6.12693791,\n",
       "        55.73982761, 102.68640076,  41.65242223, 326.00460405,\n",
       "       304.43478124,  42.88015438, 103.83633654, 120.93277589,\n",
       "        53.86453507,  56.24675112, 111.49778129,  17.48158777,\n",
       "       222.16424562,  50.94388459,  -4.92303542,  19.07002806,\n",
       "       522.6054014 ,   5.98412926,  38.67326368])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basicRidgeReg(X_train1, X_test1, y_train1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'random_state': None, 'solver': 'auto', 'tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression - Further Tuning with Grid Search\n",
    "rr = Ridge()\n",
    "# Look at parameters used by our regression\n",
    "print('Parameters currently in use:\\n')\n",
    "print(rr.get_params())\n",
    "\n",
    "\n",
    "#Creating the Random Grid\n",
    "#pipe = Pipeline([('regression model' , LinearSVR())])\n",
    "\n",
    "param_grid = [\n",
    "    {'alpha' : np.linspace(0.5,10,20),\n",
    "    'copy_X' : [True, False],\n",
    "    'fit_intercept' : [True, False],\n",
    "    'max_iter' :  [None, 5, 10, 25, 50],\n",
    "    'normalize' : [True, False],\n",
    "    'random_state' : [None, 2,3,4,5,6,7,8,9,10],\n",
    "    'solver' : ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "    'tol' : [1e-4, 2e-4]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 112000 candidates, totalling 1120000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  39 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 12017 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done 35017 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=-1)]: Done 67217 tasks      | elapsed:   22.5s\n",
      "[Parallel(n_jobs=-1)]: Done 108617 tasks      | elapsed:   39.6s\n",
      "[Parallel(n_jobs=-1)]: Done 159217 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 219017 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 288017 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 366217 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 453617 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 550217 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 656017 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 771017 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 895217 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1028617 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1120000 out of 1120000 | elapsed:  7.6min finished\n",
      "/home/nicholas/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/home/nicholas/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.5,\n",
       " 'copy_X': True,\n",
       " 'fit_intercept': True,\n",
       " 'max_iter': 5,\n",
       " 'normalize': True,\n",
       " 'random_state': None,\n",
       " 'solver': 'sag',\n",
       " 'tol': 0.0001}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model with the parameter\n",
    "reg = GridSearchCV(Ridge(), param_grid = param_grid, cv = 10, verbose=True, n_jobs=-1)\n",
    "# Fit the random search model\n",
    "best_reg= reg.fit(X_train1_scaled, y_train1)\n",
    "\n",
    "\n",
    "best_reg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned regression models\n",
    "\n",
    "def tunedRidgeRegression(X_train1, X_test1, y_train1, y_test1):\n",
    "    rigReg = Ridge(alpha = 0.5, copy_X = True, fit_intercept = True, max_iter = 100, normalize = True, solver = \"sag\", tol = 0.0001)\n",
    "    rigReg.fit(X_train1, y_train1)\n",
    "    y_pred = rigReg.predict(X_test1)\n",
    "    print(\"R2 score: \" + str(rigReg.score(X_test1, y_test1)))\n",
    "    print(\"Explained variance: \" + str(explained_variance_score(y_test1, y_pred)))\n",
    "    print(\"Max error: \" + str(max_error(y_test1, y_pred)))\n",
    "    print(\"Mean absolute error: \" + str(mean_absolute_error(y_test1, y_pred)))\n",
    "    print(\"Mean squared error: \" + str(mean_squared_error(y_test1, y_pred)))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.8153639880974343\n",
      "Explained variance: 0.8254982347782042\n",
      "Max error: 153.79691218913842\n",
      "Mean absolute error: 33.54332970475656\n",
      "Mean squared error: 2293.9239167206238\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([254.82544079,  29.19203332, 130.91047709,  49.27320611,\n",
       "        13.75125597,   7.40768724,  10.34567704,  13.01703533,\n",
       "        89.4992384 ,  36.65828625,  77.04622625, 100.9334412 ,\n",
       "        84.23629141,  23.30387509,  41.36673408,  63.63839811,\n",
       "       158.97804972,  55.49392123, 398.86346731,  52.65821276,\n",
       "         6.3613055 , 115.25657372,  82.94588406,  22.20617062,\n",
       "        29.78888203, 143.75671005,   0.96734568,  74.51057558,\n",
       "       376.17779961, 110.49339061,  23.30387509,  27.37109067,\n",
       "        73.3331846 , 113.85419125,  50.67486644, 293.79691219,\n",
       "       267.89358321,  50.4897111 ,  98.23692193, 124.90424752,\n",
       "        37.65833271,  67.16579082, 131.17758887,  32.84765537,\n",
       "       198.55286993,  62.2179525 ,   8.29213521,  29.39753338,\n",
       "       451.85147323,  18.89706049,   2.48446559])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tunedRidgeRegression(X_train1, X_test1, y_train1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tunedRidgeRegression2(X_train1, X_test1, y_train1, y_test1):\n",
    "    rigReg = Ridge(alpha = 1, copy_X = True, fit_intercept = True, normalize = True,random_state = 5, solver = \"saga\", tol = 0.0002)\n",
    "    rigReg.fit(X_train1, y_train1)\n",
    "    y_pred = rigReg.predict(X_test1)\n",
    "    print(\"R2 score: \" + str(rigReg.score(X_test1, y_test1)))\n",
    "    print(\"Explained variance: \" + str(explained_variance_score(y_test1, y_pred)))\n",
    "    print(\"Max error: \" + str(max_error(y_test1, y_pred)))\n",
    "    print(\"Mean absolute error: \" + str(mean_absolute_error(y_test1, y_pred)))\n",
    "    print(\"Mean squared error: \" + str(mean_squared_error(y_test1, y_pred)))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.8155011130039174\n",
      "Explained variance: 0.8256432098221604\n",
      "Max error: 153.74948686448943\n",
      "Mean absolute error: 33.51870396060153\n",
      "Mean squared error: 2292.2202723485507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([254.76746596,  29.13318827, 130.85869205,  49.214485  ,\n",
       "        13.73535246,   7.39941092,  10.33749879,  12.97855464,\n",
       "        89.44024356,  36.69536149,  77.0208812 , 100.90010541,\n",
       "        84.17829271,  23.53699765,  41.34179123,  63.71670096,\n",
       "       158.92048439,  55.45550309, 398.79329716,  52.62022789,\n",
       "         6.40899761, 115.22699744,  82.88402663,  22.25134572,\n",
       "        29.77497644, 143.70365922,   1.32004493,  74.44110779,\n",
       "       376.11109561, 110.44057318,  23.53699765,  27.35889345,\n",
       "        73.2945535 , 113.79223822,  50.65131079, 293.74948686,\n",
       "       267.82013088,  50.48178176,  98.17778834, 124.84909015,\n",
       "        37.93138092,  67.12653861, 131.11499033,  32.83200884,\n",
       "       198.49526296,  62.14839281,   8.34349228,  29.44336479,\n",
       "       451.79366633,  18.94374307,   2.99416187])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tunedRidgeRegression(X_train1, X_test1, y_train1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8175773170785894"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rigReg = Ridge(alpha = 1, copy_X = True, fit_intercept = True, normalize = True,random_state = 5, solver = \"saga\", tol = 0.0002)\n",
    "rigReg.fit(X_train_scaled, y_train)\n",
    "y_pred = rigReg.predict(X_test_scaled)\n",
    "saveFile(y_pred,'ridgeReg_tuned_scaled.csv')\n",
    "rigReg.score(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'alpha': 0.9, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'ls', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'presort': 'auto', 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression - Further Tuning with Grid Search\n",
    "gr = GradientBoostingRegressor()\n",
    "# Look at parameters used by our regression\n",
    "print('Parameters currently in use:\\n')\n",
    "print(gr.get_params())\n",
    "\n",
    "\n",
    "#Creating the Random Grid\n",
    "#pipe = Pipeline([('regression model' , LinearSVR())])\n",
    "\n",
    "param_grid = [\n",
    "   {#'alpha': [0.5,0.9,1.5],\n",
    "    #'criterion': 'friedman_mse', \n",
    "    #'init': None, \n",
    "    'learning_rate': [0.1, 0.5],\n",
    "    'loss': ['ls', 'lad', 'huber', 'quantile'], \n",
    "    'max_depth': [3, 5],\n",
    "    #'max_features': None, \n",
    "    #'max_leaf_nodes': None, \n",
    "    #'min_impurity_decrease': 0.0, \n",
    "    #'min_impurity_split': None, \n",
    "    #'min_samples_leaf': 1, \n",
    "    'min_samples_split': [2, 3, 5],\n",
    "    #'min_weight_fraction_leaf': 0.0, \n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    #'n_iter_no_change': None, \n",
    "    #'presort': 'auto', \n",
    "    #'random_state': None, \n",
    "    'subsample': [0.5,1.0],\n",
    "    'tol': [0.0001, 0.0002],\n",
    "    'validation_fraction': [0.1, 0.5, 1],\n",
    "    'verbose': [0,1], \n",
    "    'warm_start': [True, False]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6912 candidates, totalling 69120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 578 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1050 tasks      | elapsed:   27.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1400 tasks      | elapsed:   51.2s\n",
      "[Parallel(n_jobs=-1)]: Done 2465 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3111 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4217 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 5453 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 6784 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 8214 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 9264 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done 10414 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=-1)]: Done 11664 tasks      | elapsed: 12.1min\n",
      "[Parallel(n_jobs=-1)]: Done 13014 tasks      | elapsed: 14.8min\n",
      "[Parallel(n_jobs=-1)]: Done 14464 tasks      | elapsed: 19.1min\n",
      "[Parallel(n_jobs=-1)]: Done 16014 tasks      | elapsed: 23.4min\n",
      "[Parallel(n_jobs=-1)]: Done 17664 tasks      | elapsed: 29.2min\n",
      "[Parallel(n_jobs=-1)]: Done 19414 tasks      | elapsed: 36.3min\n",
      "[Parallel(n_jobs=-1)]: Done 21264 tasks      | elapsed: 42.2min\n",
      "[Parallel(n_jobs=-1)]: Done 23214 tasks      | elapsed: 51.0min\n",
      "[Parallel(n_jobs=-1)]: Done 25264 tasks      | elapsed: 58.8min\n",
      "[Parallel(n_jobs=-1)]: Done 27414 tasks      | elapsed: 65.7min\n",
      "[Parallel(n_jobs=-1)]: Done 29664 tasks      | elapsed: 69.0min\n",
      "[Parallel(n_jobs=-1)]: Done 32014 tasks      | elapsed: 74.7min\n",
      "[Parallel(n_jobs=-1)]: Done 34464 tasks      | elapsed: 81.8min\n",
      "[Parallel(n_jobs=-1)]: Done 38822 tasks      | elapsed: 84.5min\n",
      "[Parallel(n_jobs=-1)]: Done 43189 tasks      | elapsed: 87.1min\n",
      "[Parallel(n_jobs=-1)]: Done 45939 tasks      | elapsed: 91.9min\n",
      "[Parallel(n_jobs=-1)]: Done 48789 tasks      | elapsed: 98.4min\n",
      "[Parallel(n_jobs=-1)]: Done 51739 tasks      | elapsed: 107.4min\n",
      "[Parallel(n_jobs=-1)]: Done 54789 tasks      | elapsed: 115.4min\n",
      "[Parallel(n_jobs=-1)]: Done 57939 tasks      | elapsed: 124.3min\n",
      "[Parallel(n_jobs=-1)]: Done 61189 tasks      | elapsed: 133.5min\n",
      "[Parallel(n_jobs=-1)]: Done 64539 tasks      | elapsed: 140.0min\n",
      "[Parallel(n_jobs=-1)]: Done 67989 tasks      | elapsed: 149.1min\n",
      "[Parallel(n_jobs=-1)]: Done 69120 out of 69120 | elapsed: 153.0min finished\n",
      "/home/nicholas/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1,\n",
       " 'loss': 'huber',\n",
       " 'max_depth': 3,\n",
       " 'min_samples_split': 5,\n",
       " 'n_estimators': 500,\n",
       " 'subsample': 0.5,\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model with the parameter\n",
    "gr = GridSearchCV(GradientBoostingRegressor(), param_grid = param_grid, cv = 10, verbose=True, n_jobs=-1)\n",
    "# Fit the random search model\n",
    "best_reg= gr.fit(X_train1_scaled, y_train1)\n",
    "\n",
    "\n",
    "best_reg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True, 'criterion': 'mse', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 'warn', 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression - Further Tuning with Grid Search\n",
    "rfr = RandomForestRegressor()\n",
    "# Look at parameters used by our regression\n",
    "print('Parameters currently in use:\\n')\n",
    "print(rfr.get_params())\n",
    "\n",
    "\n",
    "#Creating the Random Grid\n",
    "#pipe = Pipeline([('regression model' , LinearSVR())])\n",
    "\n",
    "param_grid = [\n",
    "   {#'ccp_alpha': [0.0, 0.1, 0.5],\n",
    "    #'criterion': 'friedman_mse', \n",
    "    #'init': None, \n",
    "    #'bootstrap' : [True, False],\n",
    "    'max_depth': [None, 3, 5, 7],\n",
    "    'oob_score' : [True, False],\n",
    "    #'max_features': None, \n",
    "    #'max_leaf_nodes': None, \n",
    "    #'min_impurity_decrease': 0.0, \n",
    "    #'min_impurity_split': None, \n",
    "    'min_samples_leaf':[1, 2], \n",
    "    'min_samples_split': [2, 3, 4],\n",
    "    #'min_weight_fraction_leaf': 0.0, \n",
    "    'n_estimators': [100, 500, 1000, 2500, 5000],\n",
    "    #'n_iter_no_change': None, \n",
    "    #'presort': 'auto', \n",
    "    #'random_state': None, \n",
    "    #'subsample': [0.5,1.0],\n",
    "    'verbose': [0,1], \n",
    "    #'n_jobs' : -1,\n",
    "    'warm_start': [True, False]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    5.0s finished\n",
      "/home/nicholas/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 50, 'oob_score': True, 'warm_start': False}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model with the parameter\n",
    "rfr = GridSearchCV(RandomForestRegressor(), param_grid = param_grid, cv = 10, verbose=True, n_jobs=-1)\n",
    "# Fit the random search model\n",
    "best_reg= rfr.fit(X_train1_scaled, y_train1)\n",
    "\n",
    "\n",
    "best_reg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.19500134705293218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#basic ensemble regressor\n",
    "\n",
    "# random_state=1, n_estimators=10\n",
    "#reg1 = GradientBoostingRegressor(random_state=1, n_estimators=10)\n",
    "reg2 = RandomForestRegressor(random_state=1, n_estimators=10)\n",
    "#reg3 = LinearRegression()\n",
    "#reg4 = Ridge()\n",
    "reg5 = LinearSVR(max_iter = 100000)\n",
    "#reg6 = linear_model.Lasso()\n",
    "#reg7 = LogisticRegression()\n",
    "\n",
    "ereg = VotingRegressor(estimators=[('rf', reg2),('linearSVR', reg5)])\n",
    "#ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3),('ridge',reg4)\n",
    "#                                   ,('lasso', reg6),('logisticR', reg7)])\n",
    "ereg = ereg.fit(X_train, y_train)\n",
    "y_pred = ereg.predict(X_test)\n",
    "print(ereg.score(X_train_scaled, y_train))\n",
    "saveFile(y_pred,'ensemble_regressor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]0.9370709793306613\n"
     ]
    }
   ],
   "source": [
    "#tuned version of ensemble regressor\n",
    "\n",
    "#reg1 = GradientBoostingRegressor(learning_rate = 0.1, loss = 'huber', max_depth = 3,min_samples_split = 5,n_estimators = 500,\n",
    "#               subsample = 0.5, tol = 0.0001,validation_fraction = 1, verbose = 0,warm_start = False)\n",
    "reg2 = RandomForestRegressor(max_depth = None, min_samples_leaf = 1, min_samples_split = 2, n_estimators = 50,\n",
    "                             oob_score = True, verbose = 0, warm_start = True)\n",
    "#reg3 = LinearRegression()\n",
    "#reg4 = Ridge(alpha = 0.5, copy_X = True, fit_intercept = True, max_iter = 100, normalize = True, solver = \"sag\", tol = 0.0001)\n",
    "reg5 = LinearSVR(C= 100, epsilon= 0.5, fit_intercept = True, max_iter = 3000, tol = 0.0002, verbose = 1 )\n",
    "#reg6 = linear_model.Lasso()\n",
    "#reg7 = LogisticRegression()\n",
    "#reg8 = SVR(C=1, degree = 1, epsilon = 0, gamma = 1, kernel = \"linear\", max_iter = 500)\n",
    "\n",
    "\n",
    "ereg = VotingRegressor(estimators=[('rf', reg2),('linearSVR', reg5)])\n",
    "ereg = ereg.fit(X_train_scaled, y_train)\n",
    "y_pred = ereg.predict(X_test_scaled)\n",
    "print(ereg.score(X_train_scaled, y_train))\n",
    "saveFile(y_pred,'voting_regressor_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9962720592537098"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor(learning_rate = 0.1, loss = 'huber', max_depth = 3,min_samples_split = 5,n_estimators = 500,\n",
    "               subsample = 0.5, tol = 0.0001,validation_fraction = 1, verbose = 0,warm_start = False)\n",
    "gbr.fit(X_train_scaled, y_train)\n",
    "y_pred = gbr.predict(X_test_scaled)\n",
    "saveFile(y_pred,'gbr_tuned_scaled.csv')\n",
    "gbr.score(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9784110592502796"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg2 = RandomForestRegressor(max_depth = None, min_samples_leaf = 1, min_samples_split = 2, n_estimators = 100,\n",
    "                             oob_score = True, verbose = 0, warm_start = True)\n",
    "reg2.fit(X_train_scaled, y_train)\n",
    "y_pred = reg2.predict(X_test_scaled)\n",
    "saveFile(y_pred,'randomF_tuned_scaled.csv')\n",
    "reg2.score(y_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X_train1, y_train1)\n",
    "reg.score(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X_train1, y_train1)\n",
    "reg.score(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[212.  26.  74.  71.  30.  12.  21.  26.  80.  45.  33.  70.  42.  16.\n  40.  32. 274.  32. 465.  26.  69.  61.  40.  23.  19. 138.   7.  66.\n 465.  64.  12.  22.  33.  65.  35. 140. 172.  50.  46. 113.  37.  62.\n  32.  38. 100.  49.  16.  40. 510.  77.  18.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-b5ec21ddc465>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mneigh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneigh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mneigh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregression\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_check_reg_targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m         \u001b[0;31m# XXX: Remove the check in 0.23\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_reg_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/neighbors/regression.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0;34m\"precomputed kernels. Densify your matrix.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             )\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    519\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[212.  26.  74.  71.  30.  12.  21.  26.  80.  45.  33.  70.  42.  16.\n  40.  32. 274.  32. 465.  26.  69.  61.  40.  23.  19. 138.   7.  66.\n 465.  64.  12.  22.  33.  65.  35. 140. 172.  50.  46. 113.  37.  62.\n  32.  38. 100.  49.  16.  40. 510.  77.  18.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# k nearest neighbour regresion\n",
    "neigh = KNeighborsRegressor()\n",
    "neigh.fit(X_train1, y_train1)\n",
    "y_pred = neigh.predict(X_test1)\n",
    "neigh.score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
